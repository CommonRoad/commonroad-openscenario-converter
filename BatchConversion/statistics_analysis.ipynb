{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from dataclasses import fields\n",
    "from enum import Enum\n",
    "from os import path\n",
    "from typing import Optional\n",
    "from typing import Union, List, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from BatchConversion.BatchConverter import BatchConversionResult\n",
    "from OpenSCENARIO2CR.ConversionAnalyzer.AnalyzerErrorResult import AnalyzerErrorResult\n",
    "from OpenSCENARIO2CR.ConversionAnalyzer.DrivabilityChecker import DrivabilityCheckerResult\n",
    "from OpenSCENARIO2CR.ConversionAnalyzer.EAnalyzer import EAnalyzer\n",
    "from OpenSCENARIO2CR.ConversionAnalyzer.STLMonitor import STLMonitorResult\n",
    "from OpenSCENARIO2CR.ConversionAnalyzer.SpotAnalyzer import SpotAnalyzerResult\n",
    "from OpenSCENARIO2CR.Osc2CrConverter import EFailureReason\n",
    "from OpenSCENARIO2CR.Osc2CrConverterResult import Osc2CrConverterResult\n",
    "from OpenSCENARIO2CR.util.Serializable import Serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dir = \"\"\n",
    "\n",
    "with open(path.join(stats_dir, \"statistics.pickle\"), \"rb\") as stats_file:\n",
    "    # We don't need the scenario files for this analysis, just slows us down\n",
    "    Serializable.import_extra_files = False\n",
    "    all_results = pickle.load(stats_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@formatter:off\n",
    "%matplotlib inline\n",
    "#@formatter:off\n",
    "\n",
    "class EGranularity(Enum):\n",
    "    SCENARIO = 1\n",
    "    VEHICLE = 2\n",
    "\n",
    "\n",
    "def analyze_statistics(statistics: Dict[str, BatchConversionResult]):\n",
    "    counts = {}\n",
    "\n",
    "    def count(name: str, amount: int = 1):\n",
    "        if name in counts:\n",
    "            counts[name] += amount\n",
    "        else:\n",
    "            counts[name] = amount\n",
    "\n",
    "    def perc(\n",
    "            description: str,\n",
    "            part_str: Union[str, List[str]],\n",
    "            total_str: Union[str, List[str]],\n",
    "            invert: bool = False\n",
    "    ):\n",
    "        if isinstance(part_str, str):\n",
    "            part = counts.get(part_str, 0)\n",
    "        else:\n",
    "            part = sum([counts.get(single_part, 0) for single_part in part_str])\n",
    "        if isinstance(total_str, str):\n",
    "            total = counts.get(total_str, 0)\n",
    "        else:\n",
    "            total = sum([counts.get(single_total, 0) for single_total in total_str])\n",
    "        if invert:\n",
    "            part = total - part\n",
    "        if total != 0:\n",
    "            res = f\"{100 * part / total:5.1f} %\"\n",
    "        else:\n",
    "            res = \"  NaN  \"\n",
    "        print(f\"{description:<50s} {res} ({part}/{total})\")\n",
    "\n",
    "    times = []\n",
    "    rules = [rule.name for rule in fields(STLMonitorResult) if rule.type == Optional[np.ndarray]]\n",
    "\n",
    "    for scenario_path, result in statistics.items():\n",
    "        count(\"total\")\n",
    "        if not result.without_exception:\n",
    "            count(\"exception\")\n",
    "        else:\n",
    "            result = result.conversion_result\n",
    "            if isinstance(result, EFailureReason):\n",
    "                count(\"failed\")\n",
    "                count(f\"failed {result.name}\")\n",
    "            elif isinstance(result, Osc2CrConverterResult):\n",
    "                count(\"success\")\n",
    "                stats = result.statistics\n",
    "                times.append(stats.sim_time)\n",
    "                count(\"vehicle total\", stats.num_obstacle_conversions)\n",
    "                count(\"vehicle failed\", len(stats.failed_obstacle_conversions))\n",
    "                for e_analyzer, analysis in result.analysis.items():\n",
    "                    count(f\"{e_analyzer.name} scenario total\")\n",
    "                    scenario_run = True\n",
    "                    scenario_success = True\n",
    "                    rules_run = {rule: True for rule in rules}\n",
    "                    rules_success = {rule: True for rule in rules}\n",
    "                    for vehicle_name, analyzer_result in analysis.items():\n",
    "                        count(f\"{e_analyzer.name} vehicle total\")\n",
    "                        vehicle_run = True\n",
    "                        vehicle_success = True\n",
    "                        if isinstance(analyzer_result, AnalyzerErrorResult):\n",
    "                            scenario_run = False\n",
    "                            scenario_success = False\n",
    "                            vehicle_run = False\n",
    "                            vehicle_success = False\n",
    "                            for rule in rules:\n",
    "                                rules_run[rule] = False\n",
    "                                rules_success[rule] = False\n",
    "                        elif e_analyzer == EAnalyzer.STL_MONITOR:\n",
    "                            assert isinstance(analyzer_result, STLMonitorResult)\n",
    "                            for rule in rules:\n",
    "                                rule_result = getattr(analyzer_result, rule)\n",
    "                                if rule_result is None:\n",
    "                                    count(f\"{e_analyzer.name} vehicle {rule.upper()} total\")\n",
    "                                    scenario_run = False\n",
    "                                    scenario_success = False\n",
    "                                    vehicle_run = False\n",
    "                                    vehicle_success = False\n",
    "                                    rules_run[rule] = False\n",
    "                                    rules_success[rule] = False\n",
    "                                elif np.min(rule_result) <= 0.0:\n",
    "                                    count(f\"{e_analyzer.name} vehicle {rule.upper()} total\")\n",
    "                                    count(f\"{e_analyzer.name} vehicle {rule.upper()} run\")\n",
    "                                    scenario_success = False\n",
    "                                    vehicle_success = False\n",
    "                                    rules_success[rule] = False\n",
    "                                else:\n",
    "                                    count(f\"{e_analyzer.name} vehicle {rule.upper()} total\")\n",
    "                                    count(f\"{e_analyzer.name} vehicle {rule.upper()} run\")\n",
    "                                    count(f\"{e_analyzer.name} vehicle {rule.upper()} success\")\n",
    "                        elif e_analyzer == EAnalyzer.DRIVABILITY_CHECKER:\n",
    "                            assert isinstance(analyzer_result, DrivabilityCheckerResult)\n",
    "                            if not (analyzer_result.collision and analyzer_result.feasibility):\n",
    "                                scenario_success = False\n",
    "                                vehicle_success = False\n",
    "                        elif e_analyzer == EAnalyzer.SPOT_ANALYZER:\n",
    "                            assert isinstance(analyzer_result, SpotAnalyzerResult)\n",
    "                            if len(analyzer_result.predictions) != stats.num_obstacle_conversions - 1:\n",
    "                                scenario_success = False\n",
    "                                vehicle_success = False\n",
    "\n",
    "                        if vehicle_run:\n",
    "                            count(f\"{e_analyzer.name} vehicle run\")\n",
    "                        if vehicle_success:\n",
    "                            count(f\"{e_analyzer.name} vehicle success\")\n",
    "\n",
    "                    if scenario_run:\n",
    "                        count(f\"{e_analyzer.name} scenario run\")\n",
    "                    if scenario_success:\n",
    "                        count(f\"{e_analyzer.name} scenario success\")\n",
    "\n",
    "                    if e_analyzer == EAnalyzer.STL_MONITOR:\n",
    "                        for rule in rules:\n",
    "                            count(f\"{e_analyzer.name} scenario {rule.upper()} total\")\n",
    "                            if rules_run[rule]:\n",
    "                                count(f\"{e_analyzer.name} scenario {rule.upper()} run\")\n",
    "                            if rules_success[rule]:\n",
    "                                count(f\"{e_analyzer.name} scenario {rule.upper()} success\")\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "    print(f\"{'Total num scenarios':<50s} {counts['total']:5d}\")\n",
    "    print(f\"{'Average time':<50s} {np.mean(times):}\")\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(\"Granularity SCENARIO\")\n",
    "    print(\"-\" * 80)\n",
    "    perc(\"Conversion success rate\", \"success\", \"total\")\n",
    "    perc(\"Conversion failure rate\", \"failed\", \"total\")\n",
    "    for reason in EFailureReason:\n",
    "        perc(f\" | {reason.name}\", f\"failed {reason.name}\", \"failed\")\n",
    "    perc(\"Conversion exception rate\", \"exception\", \"total\")\n",
    "    for e_analyzer in EAnalyzer:\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{e_analyzer.name}\")\n",
    "        perc(\"run rate\", f\"{e_analyzer.name} scenario run\", f\"{e_analyzer.name} scenario total\")\n",
    "        perc(\"success rate\", f\"{e_analyzer.name} scenario success\", f\"{e_analyzer.name} scenario total\")\n",
    "        perc(\"\", f\"{e_analyzer.name} scenario success\", f\"{e_analyzer.name} scenario run\")\n",
    "        if e_analyzer == EAnalyzer.STL_MONITOR:\n",
    "            for rule in rules:\n",
    "                print()\n",
    "                perc(f\"{rule.upper()} run rate\", f\"{e_analyzer.name} scenario {rule.upper()} run\",\n",
    "                     f\"{e_analyzer.name} scenario {rule.upper()} total\")\n",
    "                perc(f\"{rule.upper()} success rate\", f\"{e_analyzer.name} scenario {rule.upper()} success\",\n",
    "                     f\"{e_analyzer.name} scenario {rule.upper()} total\")\n",
    "                perc(\"\", f\"{e_analyzer.name} scenario {rule.upper()} success\",\n",
    "                     f\"{e_analyzer.name} scenario {rule.upper()} run\")\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(\"Granularity VEHICLE\")\n",
    "    print(\"-\" * 80)\n",
    "    perc(\"Conversion success rate\", \"vehicle failed\", \"vehicle total\", invert=True)\n",
    "    print(\"-\" * 80)\n",
    "    for e_analyzer in EAnalyzer:\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{e_analyzer.name}\")\n",
    "        perc(\"run rate\", f\"{e_analyzer.name} vehicle run\", f\"{e_analyzer.name} vehicle total\")\n",
    "        perc(\"success rate\", f\"{e_analyzer.name} vehicle success\", f\"{e_analyzer.name} vehicle total\")\n",
    "        perc(\"\", f\"{e_analyzer.name} vehicle success\", f\"{e_analyzer.name} vehicle run\")\n",
    "        if e_analyzer == EAnalyzer.STL_MONITOR:\n",
    "            for rule in rules:\n",
    "                print()\n",
    "                perc(f\"{rule.upper()} run rate\", f\"{e_analyzer.name} vehicle {rule.upper()} run\",\n",
    "                     f\"{e_analyzer.name} vehicle {rule.upper()} total\")\n",
    "                perc(f\"{rule.upper()} success rate\", f\"{e_analyzer.name} vehicle {rule.upper()} success\",\n",
    "                     f\"{e_analyzer.name} vehicle {rule.upper()} total\")\n",
    "                perc(\"\", f\"{e_analyzer.name} vehicle {rule.upper()} success\",\n",
    "                     f\"{e_analyzer.name} vehicle {rule.upper()} run\")\n",
    "\n",
    "\n",
    "def print_exception_tracebacks(\n",
    "        statistics: Dict[str, BatchConversionResult],\n",
    "        compressed=True,\n",
    "):\n",
    "    errors: Dict[AnalyzerErrorResult, int] = {}\n",
    "    for scenario_path, result in statistics.items():\n",
    "        if not result.without_exception:\n",
    "            if not compressed:\n",
    "                print(f\"{result.conversion_result.source_file}\\n{result.exception.traceback_text}\\n\\n\\n\")\n",
    "            else:\n",
    "                errors[result.exception] = 1 + errors.get(result.exception, 0)\n",
    "\n",
    "    for error, count in errors.items():\n",
    "        print(f\"{count}\\n{error.traceback_text}\")\n",
    "        print(\"\\n\" * 3)\n",
    "\n",
    "\n",
    "def print_exception_tracebacks_for_analyzer(\n",
    "        statistics: Dict[str, BatchConversionResult], analyzer: EAnalyzer,\n",
    "        granularity: EGranularity = EGranularity.SCENARIO,\n",
    "        compressed=True,\n",
    "):\n",
    "    errors: Dict[AnalyzerErrorResult, int] = {}\n",
    "    for scenario_path, result in statistics.items():\n",
    "        if result.without_exception and isinstance(result.conversion_result, Osc2CrConverterResult):\n",
    "            analysis = result.conversion_result.analysis\n",
    "            if analyzer in analysis:\n",
    "                error = None\n",
    "                for vehicle_name, analyzer_result in analysis[analyzer].items():\n",
    "                    if isinstance(analyzer_result, AnalyzerErrorResult):\n",
    "                        error = analyzer_result\n",
    "                        if granularity == EGranularity.VEHICLE:\n",
    "                            if not compressed:\n",
    "                                print(f\"{result.conversion_result.source_file}\\n{analyzer_result.traceback_text}\")\n",
    "                            else:\n",
    "                                errors[analyzer_result] = 1 + errors.get(analyzer_result, 0)\n",
    "                if granularity == EGranularity.SCENARIO and error is not None:\n",
    "                    if not compressed:\n",
    "                        print(f\"{result.conversion_result.source_file}\\n{error.traceback_text}\\n\\n\\n\")\n",
    "                    else:\n",
    "                        errors[error] = 1 + errors.get(error, 0)\n",
    "\n",
    "    for error, count in errors.items():\n",
    "        print(f\"{count}\\n{error.exception_text}\\n{error.traceback_text}\")\n",
    "        print(\"\\n\" * 3)\n",
    "\n",
    "\n",
    "def plot_times(results, n_bins: int = 25, low_pass_filter: float = 50.0):\n",
    "    times = []\n",
    "    for scenario_path, result in results.items():\n",
    "        if result.without_exception and isinstance(result.conversion_result, Osc2CrConverterResult):\n",
    "            times.append(result.conversion_result.statistics.sim_time)\n",
    "    fig, axs = plt.subplots(1, 2, sharey=\"row\", tight_layout=True, figsize=(7,2.5))\n",
    "    axs[0].hist(times, bins=n_bins)\n",
    "    axs[0].set_xlabel(\"t [s]\")\n",
    "    axs[0].set_ylabel(\"# scenarios\")\n",
    "\n",
    "    axs[1].hist([t for t in times if t <= low_pass_filter], bins=n_bins)\n",
    "    axs[1].set_xlabel(\"t [s]\")\n",
    "    axs[1].set_ylabel(\"# scenarios\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_num_obstacles(results, low_pass_filter: int = 10):\n",
    "    values = []\n",
    "    for scenario_path, result in results.items():\n",
    "        if result.without_exception and isinstance(result.conversion_result, Osc2CrConverterResult):\n",
    "            values.append(result.conversion_result.statistics.num_obstacle_conversions)\n",
    "    fig, axs = plt.subplots(1, 2, sharey=\"row\", tight_layout=True, figsize=(7,2.5))\n",
    "\n",
    "    x = list(range(max(values) + 1))\n",
    "    y = [len([None for v in values if v == x_val]) for x_val in x]\n",
    "    axs[0].bar(x, y)\n",
    "    axs[0].set_xlabel(\"# obstacles\")\n",
    "    axs[0].set_ylabel(\"# scenarios\")\n",
    "\n",
    "    filtered = [v for v in values if v <= low_pass_filter]\n",
    "    x = list(range(max(filtered) + 1))\n",
    "    y = [len([None for v in filtered if v == x_val]) for x_val in x]\n",
    "    axs[1].bar(x, y)\n",
    "    axs[1].set_xlabel(\"# obstacles\")\n",
    "    axs[1].set_ylabel(\"# scenarios\")\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All Scenarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze_statistics(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_exception_tracebacks(imported_results, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_exception_tracebacks_for_analyzer(imported_results, EAnalyzer.DRIVABILITY_CHECKER, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_exception_tracebacks_for_analyzer(imported_results, EAnalyzer.STL_MONITOR, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_exception_tracebacks_for_analyzer(imported_results, EAnalyzer.SPOT_ANALYZER, compressed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ASAM OpenSCENARIO Example Scenarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_results = {f: r for f, r in all_results.items() if \"openscenario-v1.1.1/Examples\" in f}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyze_statistics(filtered_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print_exception_tracebacks(imported_results, compressed=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print_exception_tracebacks_for_analyzer(imported_results, EAnalyzer.DRIVABILITY_CHECKER, compressed=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print_exception_tracebacks_for_analyzer(imported_results, EAnalyzer.STL_MONITOR, compressed=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print_exception_tracebacks_for_analyzer(imported_results, EAnalyzer.SPOT_ANALYZER, compressed=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_times(filtered_results)\n",
    "\n",
    "plot_num_obstacles(filtered_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
