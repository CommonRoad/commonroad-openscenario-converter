{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataclasses import fields\n",
    "from os import path\n",
    "from typing import Union, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from OpenSCENARIO2CR.ConversionAnalyzer.EAnalyzer import EAnalyzer\n",
    "from OpenSCENARIO2CR.ConversionAnalyzer.ErrorAnalysisResult import ErrorAnalysisResult\n",
    "from OpenSCENARIO2CR.ConversionAnalyzer.STLMonitor import STLMonitorResult\n",
    "from OpenSCENARIO2CR.Osc2CrConverter import EFailureReason\n",
    "from OpenSCENARIO2CR.util.ConversionStatistics import ConversionStatistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats_dir = \"results/\"\n",
    "\n",
    "with open(path.join(stats_dir, \"statistics.yml\"), \"r\") as stats_file:\n",
    "    imported_statistics = yaml.safe_load(stats_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def analyze_statistics(statistics: Dict):\n",
    "    counts = {}\n",
    "\n",
    "    def count(name: str, amount: int = 1):\n",
    "        if name in counts:\n",
    "            counts[name] += amount\n",
    "        else:\n",
    "            counts[name] = amount\n",
    "\n",
    "    def perc(\n",
    "            description: str,\n",
    "            part_str: Union[str, List[str]],\n",
    "            total_str: Union[str, List[str]],\n",
    "            invert: bool = False\n",
    "    ):\n",
    "        if isinstance(part_str, str):\n",
    "            part = counts.get(part_str, 0)\n",
    "        else:\n",
    "            part = sum([counts.get(single_part, 0) for single_part in part_str])\n",
    "        if isinstance(total_str, str):\n",
    "            total = counts.get(total_str, 0)\n",
    "        else:\n",
    "            total = sum([counts.get(single_total, 0) for single_total in total_str])\n",
    "        if invert:\n",
    "            part = total - part\n",
    "        if total != 0:\n",
    "            res = f\"{100 * part / total:5.1f} %\"\n",
    "        else:\n",
    "            res = \"  NaN  \"\n",
    "        print(f\"{description:<50s} {res} ({part}/{total})\")\n",
    "\n",
    "    times = []\n",
    "    rules = [rule.name for rule in fields(STLMonitorResult)]\n",
    "\n",
    "    for scenario_path, result in statistics.items():\n",
    "        count(\"total\")\n",
    "        if result[0] == \"success\":\n",
    "            count(\"success\")\n",
    "            stats = ConversionStatistics.from_dict(result[1])\n",
    "            times.append(stats.sim_time)\n",
    "            count(\"obstacle_total\", stats.num_obstacle_conversions)\n",
    "            count(\"obstacle_failed\", len(stats.failed_obstacle_conversions))\n",
    "            if EAnalyzer.STL_MONITOR in stats.analysis:\n",
    "                count(\"compliance_total_scenario\")\n",
    "                scenario_run = True\n",
    "                scenario_compliant = True\n",
    "                rules_run = {rule: True for rule in rules}\n",
    "                rules_compliant = {rule: True for rule in rules}\n",
    "                for vehicle_name, all_monitor_result in stats.analysis[EAnalyzer.STL_MONITOR].items():\n",
    "                    count(\"compliance_total_vehicle\")\n",
    "                    vehicle_run = True\n",
    "                    vehicle_compliant = True\n",
    "                    for rule in rules:\n",
    "                        count(f\"compliance_total_vehicle_\" + rule.upper())\n",
    "                        if isinstance(all_monitor_result, ErrorAnalysisResult) or getattr(all_monitor_result,\n",
    "                                                                                          rule) is None:\n",
    "                            scenario_run = False\n",
    "                            scenario_compliant = False\n",
    "                            vehicle_run = False\n",
    "                            vehicle_compliant = False\n",
    "                            rules_run[rule] = False\n",
    "                            rules_compliant[rule] = False\n",
    "                            continue\n",
    "                        count(\"compliance_run_vehicle_\" + rule.upper())\n",
    "                        if all([value >= 0.0 for value in getattr(all_monitor_result, rule)]):\n",
    "                            count(\"compliance_compliant_vehicle_\" + rule.upper())\n",
    "                        else:\n",
    "                            scenario_compliant = False\n",
    "                            vehicle_compliant = False\n",
    "                            rules_compliant[rule] = False\n",
    "                    if vehicle_run:\n",
    "                        count(\"compliance_run_vehicle\")\n",
    "                    if vehicle_compliant:\n",
    "                        count(\"compliance_compliant_vehicle\")\n",
    "                if scenario_run:\n",
    "                    count(\"compliance_run_scenario\")\n",
    "                if scenario_compliant:\n",
    "                    count(\"compliance_compliant_scenario\")\n",
    "\n",
    "                for rule in rules:\n",
    "                    count(\"compliance_total_scenario_\" + rule.upper())\n",
    "                    if rules_run[rule]:\n",
    "                        count(\"compliance_run_scenario_\" + rule.upper())\n",
    "                    if rules_compliant[rule]:\n",
    "                        count(\"compliance_compliant_scenario_\" + rule.upper())\n",
    "\n",
    "\n",
    "        elif result[0] == \"failure\":\n",
    "            count(\"failed\")\n",
    "            count(f\"failed_{result[1]}\")\n",
    "        elif result[0] == \"error\":\n",
    "            count(\"error\")\n",
    "\n",
    "    print(f\"{'Total num scenarios':<50s} {counts['total']:5d}\")\n",
    "    print(f\"{'Average time':<50s} {np.mean(times):}\")\n",
    "    print(\"-\" * 58)\n",
    "    perc(\"Conversion success rate\", \"success\", \"total\")\n",
    "    perc(\"Conversion failure rate\", \"failed\", \"total\")\n",
    "    for reason in EFailureReason:\n",
    "        perc(f\" | {reason.name}\", f\"failed_{reason.name}\", \"failed\")\n",
    "    perc(\"Conversion error rate\", \"error\", \"total\")\n",
    "    print(\"-\" * 58)\n",
    "    perc(\"Obstacle conversion rate\", \"obstacle_failed\", \"obstacle_total\", invert=True)\n",
    "    print(\"-\" * 58)\n",
    "    print(\"CommonRoad STL Monitor Results, granularity: scenario\")\n",
    "    perc(\"run rate\", \"compliance_run_scenario\", \"compliance_total_scenario\")\n",
    "    perc(\"compliant rate of total\", \"compliance_compliant_scenario\", \"compliance_total_scenario\")\n",
    "    perc(\"compliant rate of run\", \"compliance_compliant_scenario\", \"compliance_run_scenario\")\n",
    "    for rule in rules:\n",
    "        perc(f\"{rule.upper()} run rate\", f\"compliance_run_scenario_{rule.upper()}\",\n",
    "             f\"compliance_total_scenario_{rule.upper()}\")\n",
    "        perc(f\"{rule.upper()} compliant rate of total\", f\"compliance_compliant_scenario_{rule.upper()}\",\n",
    "             f\"compliance_total_scenario_{rule.upper()}\")\n",
    "        perc(f\"{rule.upper()} compliant rate of run\", f\"compliance_compliant_scenario_{rule.upper()}\",\n",
    "             f\"compliance_run_scenario_{rule.upper()}\")\n",
    "    print(\"-\" * 58)\n",
    "    print(\"CommonRoad STL Monitor Results, granularity: vehicle\")\n",
    "    perc(\"run rate\", \"compliance_run_vehicle\", \"compliance_total_vehicle\")\n",
    "    perc(\"compliant rate of total\", \"compliance_compliant_vehicle\", \"compliance_total_vehicle\")\n",
    "    perc(\"compliant rate of run\", \"compliance_compliant_vehicle\", \"compliance_run_vehicle\")\n",
    "    for rule in rules:\n",
    "        perc(f\"{rule.upper()} run rate\", f\"compliance_run_vehicle_{rule.upper()}\",\n",
    "             f\"compliance_total_vehicle_{rule.upper()}\")\n",
    "        perc(f\"{rule.upper()} compliant rate of total\", f\"compliance_compliant_vehicle_{rule.upper()}\",\n",
    "             f\"compliance_total_vehicle_{rule.upper()}\")\n",
    "        perc(f\"{rule.upper()} compliant rate of run\", f\"compliance_compliant_vehicle_{rule.upper()}\",\n",
    "             f\"compliance_run_vehicle_{rule.upper()}\")\n",
    "\n",
    "\n",
    "def print_exception_tracebacks(statistics: Dict, compressed=True):\n",
    "    errors: Dict[ErrorAnalysisResult, int] = {}\n",
    "    for scenario_path, result in statistics.items():\n",
    "        if result[0] == \"error\":\n",
    "            error = ErrorAnalysisResult.from_dict(result[1])\n",
    "            if not compressed:\n",
    "                print(f\"{scenario_path}\\n{error.traceback_text}\")\n",
    "                print(\"\\n\" * 3)\n",
    "            elif error in errors:\n",
    "                errors[error] += 1\n",
    "            else:\n",
    "                errors[error] = 1\n",
    "\n",
    "    for error, count in errors.items():\n",
    "        print(f\"{count}\\n{error.traceback_text}\")\n",
    "        print(\"\\n\" * 3)\n",
    "\n",
    "\n",
    "def print_exception_tracebacks_for_analyzer(statistics: Dict, analyzer: EAnalyzer, compressed=True):\n",
    "    errors: Dict[ErrorAnalysisResult, int] = {}\n",
    "    for scenario_path, result in statistics.items():\n",
    "        if scenario_path == \"results/2022-09-21_08:32:45/ALKS_Scenario_4.6_2_LateralDetectionRange_TEMPLATE\":\n",
    "            print(\"A\")\n",
    "        if result[0] == \"success\":\n",
    "            stats = ConversionStatistics.from_dict(result[1])\n",
    "            if analyzer in stats.analysis:\n",
    "                for vehicle_name, analyzer_result in stats.analysis[analyzer].items():\n",
    "                    if isinstance(analyzer_result, ErrorAnalysisResult):\n",
    "                        if not compressed:\n",
    "                            print(f\"{stats.source_file}\\n{analyzer_result.traceback_text}\")\n",
    "                        elif analyzer_result in errors:\n",
    "                            errors[analyzer_result] += 1\n",
    "                        else:\n",
    "                            errors[analyzer_result] = 1\n",
    "\n",
    "    for error, count in errors.items():\n",
    "        print(f\"{count}\\n{error.traceback_text}\")\n",
    "        print(\"\\n\" * 3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyze_statistics(imported_statistics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_exception_tracebacks(imported_statistics, compressed=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_exception_tracebacks_for_analyzer(imported_statistics, EAnalyzer.STL_MONITOR, compressed=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
